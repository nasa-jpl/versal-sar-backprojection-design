% =============================================================================
\section{Implementation}
% =============================================================================
\label{sec:impl}

% =============================================================================
\subsection{Algorithmic Basis and Dataset}
% =============================================================================

This design builds upon the mathematical formulations presented in the "SAR
Image Formation Toolbox for MATLAB" by Gorham et al.~\cite{Gorham}. The toolbox
implements a backprojection algorithm in spotlight mode using the "Gotcha
Volumetric SAR Data Set, Version 1.0" as its primary example~\cite{GOTCHA}.
This dataset, collected at X-band with a 640~MHz bandwidth, includes full
azimuth coverage across eight elevation angles and complete polarization. The
imaging scene contains civilian vehicles and calibration targets, making it a
standard benchmark for validating SAR image-formation methods.

The dataset is distributed in MATLAB binary format (*.mat). Each file
represents one degree of azimuth for a single pass and polarization, and
contains phase-history samples, frequency vectors, antenna coordinates (X, Y,
Z), range to scene center, and azimuth/elevation angles. These measurements
supply both the raw radar returns and the platform geometry needed to drive the
backprojection reconstruction.

In this work, Gorhamâ€™s mathematical formulation and dataset are mapped onto
Versal hardware to evaluate parallelization strategies. Focused images produced
by the AI Engine implementation are compared against the MATLAB reference
outputs. Appendix \ref{sec:matlab_appendix} contains more information on how the
MATLAB code was used and how the dataset was used to generate the input data
for the AI Engine.

The following sections describe how this algorithm and dataset are mapped onto
the Versal ACAP architecture, including the division of responsibilities across
the AI Engine, programmable logic, and ARM cores.

% =============================================================================
\subsection{Algorithm Mapping to Hardware}
% =============================================================================

The Versal architecture was used to accelerate the Gorham backprojection
formulation. Typical acceleration methods on the Versal are expressed through
SIMD intrinsics, multi-core instantiation, and ILP. The current implementation
heavily emphasizes SIMD and multi-core parallelism. While some ILP
optimizations were implemented, further ILP modifications could potentially be
made and are reserved for future work (see
Section~\ref{sec:ilp_optimizations}). Accuracy is validated by comparing
focused images from the Versal hardware and AI Engine simulations against
MATLAB reference outputs.

During development, multiple custom AI Engine and PL kernel designs were
evaluated. The finalized design includes three types of AI Engine kernels and
up to two PL kernels (depending on the build branch described in
Section~\ref{sec:system_overview}). These kernels can be instantiated a
specific number of times based on the build configuration. The summary of
these kernels, along with how many times the they can be instantiated in the
current design, are listed below:

\subsubsection{AI Engine Kernels}
\begin{itemize}
    \item \textbf{Data Broadcast Kernel (1x):} Distributes slow-time radar data
        (antenna coordinates, range-to-scene center) and range-compressed
        samples to all Image Reconstruction kernels.
    \item \textbf{Pixel Demux Kernel (1x-7x):} Assigns target pixels of
        interest to the Image Reconstruction kernels. In the \textit{main}
        branch, this kernel also performs input sorting. In
        \textit{host\_stride} and \textit{pl\_stride}, sorting is performed by
        the ARM or PL, respectively.
    \item \textbf{Image Reconstruction Kernel (1x-224x):} Implements the core 
        backprojection loop. For each pixel, it computes the differential 
        range relative to the scene center, applies phase correction, and 
        accumulates contributions across pulses. These kernels exploit SIMD 
        vectorization to maximize throughput.
\end{itemize}

\subsubsection{PL Kernels}
\begin{itemize}
    \item \textbf{DMA Packet Router Kernel (1x-7x):} Reorders merged
        AXI4-Stream outputs from the AI Engine into contiguous memory in DDR,
        ensuring that image pixels are correctly placed for reconstruction.
    \item \textbf{DMA Stride Controller Kernel (1x-7x):} Reads and reorders
        specific data segments from DDR before streaming them into the AI
        Engine, offloading the input-sorting responsibility from the ARM or AI
        Engine.
\end{itemize}

These kernels form the building blocks of the overall processing chain. A
scaled-down version of the design showing how these kernels interconnect are
displayed in Figure~\ref{fig:aie_graph}. The next section presents the
system-level overview and how data flows and interacts through the system as a
whole.

% =============================================================================
\subsection{System Overview}
% =============================================================================
\label{sec:system_overview}

The design integrates the ARM Cortex-A72 processor, the AI Engine array, and
the PL. At a high-level, the ARM Cortex-A72 is responsible for the following
tasks:

\begin{itemize}
    \item Retrieves input data from non-volatile memory and orchestrates
        dataflow throughout the system.
    \item Manages PL kernel and AI Engine graph control execution.
    \item Configures kernel parameters.
    \item Records performance metrics.
\end{itemize}

\noindent The AI Engine array is responsible for the following tasks:

\begin{itemize}
    \item Parallelizes the backprojection algorithm across several AI kernels.
    \item Vectorizes the backprojection algorithm on each AI kernel.
    \item Performs minimal ILP through for loop pipelining.
    \item Sorts pixels into specific AI kernels (based on which branch of the
        code is being ran).
    \item Focuses image data per pulse until the ARM processor signals an RTP
        signal.
\end{itemize}

\noindent The PL acts as a custom DMA controller and is responsible for the
following tasks:

\begin{itemize}
    \item Post-processing the data after AI Engine egress by reordering packets
        into DDR.
    \item Pre-processing the data before AI Engine ingress by reordering
        packets from DDR (based on which branch of the code is being ran).
\end{itemize}

\noindent There are three branch configurations to compare performance
trade-offs:

\begin{itemize}
    \item \textbf{main:} The AI Engine kernels ingest unsorted data and perform 
        their own input reordering before computation.
    \item \textbf{host\_stride:} The ARM Cortex-A72 pre-sorts the data in DDR, 
        reducing the sorting work inside the AI Engine.
    \item \textbf{pl\_stride:} The PL handles the pre-processing by implementing
        a DMA stride controller that pre-sorts the data before it reaches the
        AI Engine.
\end{itemize}

Regardless if pre-processing is handled in the AI Engine, ARM, or PL, the
post-processing is always handled within the PL logic. This is required because
packet stream mergers are used inside the AI Engine to collapse many kernel
outputs into the limited set of GMIO ports (32 input and 32 output). While
merging improves GMIO port utilization, it scrambles output order. A PL packet
router therefore inspects packet IDs on the AXI streams and reorders them into
a contiguous layout in DDR, so the final image is coherent without additional
post-processing work by the ARM processor.

% -----------------------------------------------------------------------------
% Default System Architecture
% -----------------------------------------------------------------------------
\begin{figure}[!hbtp]
  \begin{center}
    \includegraphics[width=\textwidth]{figures/default_system_arch.png}
  \end{center}
  \caption{System architecture for the \textit{main} and \textit{host\_stride}
      branches. The ARM loads data to DDR (unsorted in \textit{main},
      pre-sorted in \textit{host\_stride}), which is then sent through the NoC
      to AI Engine backprojection clusters. When the ARM determines the image
      is sufficiently focused, it issues an RTP signal over the NoC directing
      the clusters to emit their fully focused image data. The results are
      streamed to the PL packet router, which reorders merged outputs and
      writes them
  contiguously to DDR.}
  \label{fig:default_system_arch}
\end{figure}
% -----------------------------------------------------------------------------

As shown in Figure~\ref{fig:default_system_arch}, all data transactions between
compute domains traverse the NoC. Range-compressed radar pulses are delivered
to the AI Engine through the NoC, and for each pulse an RTP signal is also
issued from the ARM to indicate whether the kernels should continue cumulative
summing across pulses or flush their accumulated results. When a flush is
triggered, the fully focused image data is emitted via AXI4-Stream into the PL
packet router, which reorders the merged streams and writes the results
contiguously to DDR.

\newpage

% -----------------------------------------------------------------------------
% PL Stride Controller System Architecture
% -----------------------------------------------------------------------------
\begin{figure}[!hbtp]
  \begin{center}
    \includegraphics[width=\textwidth]{figures/pl_stride_system_arch.png}
  \end{center}
  \caption{System architecture for the \textit{pl\_stride} branch. The ARM
      loads data to DDR, and the PL stride controller selects and reorders the
      required segments before forwarding them through the NoC to AI Engine
      backprojection clusters. When the ARM determines the image is
      sufficiently focused, it issues an RTP signal over the NoC directing the
      clusters to emit their fully focused image data. The results are streamed
      to the PL packet router, which reorders merged outputs and writes them
      contiguously to DDR.}
  \label{fig:pl_stride_system_arch}
\end{figure}
% -----------------------------------------------------------------------------

For the \textit{pl\_stride} architecture shown in
Figure~\ref{fig:pl_stride_system_arch}, the PL takes on the input-side sorting
responsibility. Instead of the ARM or AI Engine performing data reordering, the
PL stride controller reads specific segments from DDR and delivers them
directly to the targeted backprojection clusters in the AI Engine. This
eliminates sorting overhead in the ARM software or AI kernels and leverages
dedicated DMA capabilities in the PL. Once inside the AI Engine backprojection
kernels, computation proceeds as in the other branches: results are packetized,
passed to the PL packet router, and written to DDR in the correct order for the
ARM to retrieve a fully focused image.

The dataflow described here is expressed in the AIE graph programming model,
which manages kernel interconnects, buffering, and synchronization. At a high
level, the graph provides constructs such as broadcasting (sends specific data
from one kernel to many), packet splitters (dividing a stream into multiple
paths), and packet mergers (combining multiple streams into one). These
abstractions allow the toolchain to map kernel interconnects onto the
underlying AXI4-Stream fabric without requiring manual stream management
software. The specific kernel implementations and the overall graph topology
are described in more detail in the following sections.

\FloatBarrier

\newpage

% =============================================================================
\subsection{AI and PL Kernels}
% =============================================================================
\label{subsec:ai_pl_kernels}

% -----------------------------------------------------------------------------
% Simplified AI Engine Graph w/o PL DMA stride controller for pre-processing
% -----------------------------------------------------------------------------
\begin{figure}[hbtp]
  \begin{center}
    \includegraphics[width=1\textwidth]
    {figures/system_2switch_2aie.png}
  \end{center}
  \caption{A scaled-down version of the design showing the dataflow between the
      AI Engine and the PL kernels. There is a total of 1 Data Broadcaster, 2
      Pixel Demuxers, and 4 Image Reconstruction AI kernels, along with 2 DMA
      Packet Router PL kernels for AI Engine post processing.}
  \label{fig:aie_graph}
\end{figure}
% -----------------------------------------------------------------------------

Figure~\ref{fig:aie_graph} depicts a scaled-down version of the dataflow
between the AI Engine and the PL kernels. This design comes from the
\textit{main} or \textit{host\_stride} branches where the following
configurations within \textit{design/common.h} were set:

\begin{itemize}
    \item $\mathtt{PULSES} = 1$: Number of pulses to process per iteration.
    \item $\mathtt{RC\_SAMPLES} = 64$: Number of range compression samples to
        process per iteration.
    \item $\mathtt{AIE\_SWITCHES} = 2$: The number of AXI4-Stream switch
        splitter/merger pairs. The compute units that contain the \gls{aie}
        Switches, Image Reconstruction kernels, and Pixel Demux kernel are
        known as \texttt{bpCluster}'s, as shown in Figure~\ref{fig:aie_graph}.
    \item $\mathtt{IMG\_SOLVERS\_PER\_SWITCH} = 2$: The number of Image
        Reconstruction kernels per \texttt{bpCluster}.
\end{itemize}

The maximum number of AXI4-Stream channels per packet spliter or merger is 32.
This means up to 32 Image Reconstruction kernels can exist per packet
spliter/merger pair (or \texttt{bpCluster}). The above diagram only depicts two
Image Reconstruction kernels per \texttt{bpCluster}. Only one Pixel Demux AI
kernel and DMA Packet Router PL kernel can exist per \texttt{bpCluster} (as
depicted in Figure~\ref{fig:aie_graph}).


\FloatBarrier

% =============================================================================
\subsubsection{Data Broadcast Kernel}
% =============================================================================
\label{subsec:data_broadcasting_kern}

There is only one Data Broadcast kernel that needs to be instantiated in the
design. The Data Broadcast kernel receives its data from the ARM processor via
DDR through two GMIO ports (slowtime\_in and rc\_in) and it broadcasts its data
via two output ports (slowtime\_out and rc\_out) feeding into the Image
Reconstruction kernel. The slowtime\_in and slowtime\_out ports transfer
AXI4-Stream data and rc\_in and rc\_out ports transfer buffered ping-pong data.
The inputs and outputs for the Data Broadcast kernel are listed below:

\begin{itemize}
    \item \textbf{slowtime\_in (float):} AXI4-Stream from the DDR
        originating from the ARM processor that contains antenna X, Y, Z and
        range to scene center.
    \item \textbf{rc\_in (\texttt{RC\_SAMPLES} cfloats):} Buffered range
        compressed phase data from the DDR originating from the ARM processor.
    \item \textbf{slowtime\_out (float):} AXI4-Stream that contains antenna
        X, Y, Z and range to scene center. This stream is broadcasted to every
        instantiated Image Reconstruction kernel.
    \item \textbf{rc\_out (\texttt{RC\_SAMPLES} cfloats):} Buffered range
        compressed phase data that is broadcasted to every instantiated Image
        Reconstruction kernel.
\end{itemize}

As mentioned in Section \ref{subsec:aie_graphs}, execution of kernels begin
when they receive their specified number of data samples matching their
allocated buffer size (assuming the kernel uses any buffer types as input). The
Broadcast kernel only has one buffer input, which is \texttt{rc\_in} (shown in
Figure~\ref{fig:aie_graph}). The \texttt{rc\_in} buffer holds up to
\texttt{RC\_SAMPLES} cfloats, where each cfloat is 8 bytes in total (2 floats
for the real and imaginary numbers). Once \texttt{rc\_in} buffer is completely
filled, the kernel will execute. Currently, \texttt{RC\_SAMPLES} can only be
64, 128, 256, or 512 cfloats. 512 cfloats is the maximum size due to buffer
size limitations within the AI Engine. Once the Data Broadcast kernel executes,
it performs the following steps:

\begin{enumerate}
    \item Perform a SIMD vector operation that pops 4 floats from the
        \texttt{slowtime\_in} AXI4-Stream port and pushes it onto the
        \texttt{slowtime\_out} AXI4-Stream port. This may seem redundant, but
        it is needed to exploit the broadcasting mechanisms within the AI
        Engine to broadcast the \texttt{slowtime\_out} AXI4-Stream data to all
        of the Image Reconstruction kernels.
    \item For a total of \texttt{RC\_SAMPLES}/16, perform a SIMD vector
        operation that pops 16 cfloats at a time from the \texttt{rc\_in}
        ping-pong buffer port and push them onto the \texttt{rc\_out} ping-pong
        buffer port.
    \item Once the \texttt{rc\_out} ping-pong buffer has a total of
        \texttt{RC\_SAMPLES} cfloats, the Broadcast kernel now has a full
        \texttt{rc\_out} buffer so it will automatically release lock on the
        \texttt{rc\_out} buffer port and forward that data out from the
        Broadcast kernel's local memory to all Image Reconstruction kernel
        memories via an AXI4-Stream burst.
\end{enumerate}

\FloatBarrier

\newpage

% =============================================================================
\subsubsection{Pixel Demux Kernel}
% =============================================================================
\label{subsec:pixel_demux_kern}

Only one Pixel Demux kernel is instantiated per \texttt{bpCluster}, as shown in
Figure~\ref{fig:aie_graph}. The Pixel Demux kernel receives its data from the
ARM processor via DDR through one GMIO port (\texttt{px\_xyz\_in}). It pushes
its data out via one output port (\texttt{px\_xyz\_out}) that feeds into an
AXI4-Stream splitter switch. The port types for \texttt{px\_xyz\_in} and
\texttt{px\_xyz\_out} are both AXI4-Stream data ports. The input and output
port descriptions for the Pixel Demuxer kernel are listed below:

\begin{itemize}
    \item \textbf{px\_xyz\_in (float):} AXI4-Stream from the DDR originating
        from the ARM processor that contains X, Y and Z location of the target
        pixels of interest with respect to the target image center.
    \item \textbf{px\_xyz\_out (float):} AXI4-Stream packets that get fed to a
        AXI4-Stream packet splitter to distribute among all Image
        Reconstruction kernels in a \texttt{bpCluster}. The maximum number of
        AXI4-Stream packets that the splitter can handle, and therefore the
        maximum number of Image Reconstruction kernels, is 32.
\end{itemize}

\noindent The following outlines the Pixel Demux operations based on the
\textit{main} branch after kernel execution:

\begin{enumerate}
    \item \label{itm:create_header_1} Construct an AXI4-Stream packet header and
        send it out the \texttt{px\_xyz\_out} AXI4-Stream port.
    \item \label{itm:px_in_px_out_1} Read a float from \texttt{px\_xyz\_in}
        AXI4-Stream and send it out through \texttt{px\_xyz\_out} AXI4-Stream.
    \item \label{itm:demux_repeat_1} Repeat step \ref{itm:px_in_px_out_1} for
        $(3*PULSES*RC\_SAMPLES)/IMG\_SOLVERS$ times.
    \item Repeat steps \ref{itm:create_header_1} through
        \ref{itm:demux_repeat_1} for \texttt{IMG\_SOLVERS\_PER\_SWITCH} times.
\end{enumerate}

\noindent The following outlines the Pixel Demux operations based on the
\textit{host\_stride} branch after kernel execution:

\begin{enumerate}
    \item \label{itm:create_header_2} Construct an AXI4-Stream packet header
        and send it out the \texttt{px\_xyz\_out} AXI4-Stream port.
    \item \label{itm:px_in_px_out_2} Read a float from \texttt{px\_xyz\_in}
        AXI4-Stream and send it out through \texttt{px\_xyz\_out} AXI4-Stream.
    \item \label{itm:demux_inner_repeat_2} Repeat step \ref{itm:px_in_px_out_2}
        for $48$ times since the Image Reconstruction kernel expects to receive
        16 floats of X, Y and Z at a time ($16*3=48$).
    \item \label{itm:demux_middle_repeat_2} Repeat steps
        \ref{itm:create_header_2} through \ref{itm:demux_inner_repeat_2} for
        \texttt{IMG\_SOLVERS\_PER\_SWITCH} times.
    \item Repeat steps \ref{itm:create_header_2} through
        \ref{itm:demux_middle_repeat_2} for \texttt{PULSES} times.
\end{enumerate}


\FloatBarrier

\newpage
% -----------------------------------------------------------------------------
% Simplified AI Engine Graph with PL DMA stride controller for pre-processing
% -----------------------------------------------------------------------------
\begin{figure}[hbtp]
  \begin{center}
    \includegraphics[width=1\textwidth]
    {figures/system_1switch_4aie.png}
  \end{center}
  \caption{A scaled-down version of the design showing dataflow between the AI
      Engine and the PL kernels. There is a total of 1 Data Broadcaster, 1
      Pixel Demuxer, and 4 Image Reconstruction AI kernels, along with 1 DMA
      Packet Router PL kernel for AI Engine post-processing and 1 DMA Stride
  Controller PL kernel for AI Engine pre-processing.}
  \label{fig:aie_graph_w_stride_ctl}
\end{figure}
% -----------------------------------------------------------------------------

The \textit{pl\_stride} branch has a different architecture than the
\textit{main} or \textit{host\_stride} branches since it uses a DMA Stride
controller in the PL for pre-processing before data is sent to the AI Engine.
Figure~\ref{fig:aie_graph_w_stride_ctl} depicts a scaled-down version of the
dataflow between the AI Engine and the PL kernels. The following outlines the
Pixel Demux operations based on the \textit{pl\_stride} branch after kernel
execution:

\begin{enumerate}
    \item \label{itm:create_header_3} Construct an AXI4-Stream packet header and
        send it out the \texttt{px\_xyz\_out} AXI4-Stream port
    \item \label{itm:px_in_px_out_3} Perform a SIMD vector operation that pops
        4 floats (X, Y, Z, and a zero padded number) from the
        \texttt{px\_xyz\_in} AXI4-Stream port and send them out through the
        \texttt{px\_xyz\_out} AXI4-Stream port. The reason a zero pad is used
        for the 4th float is because the PL optimizes its transfers by sending
        128bit bursts at a time. The first 96 bits (12 bytes) are the X, Y, and
        Z target pixels. To keep the data transactions aligned with how the PL
        fetches data from DDR 64 bits at a time, the last 32 bits (4 bytes) are
        zero padded.
    \item \label{itm:demux_repeat_3} Repeat step \ref{itm:px_in_px_out_3} for
        $16$ times since the Image Reconstruction kernel expects to receive 16
        X, Y and Z components at a time.
    \item \label{itm:demux_middle_repeat_3} Repeat steps
        \ref{itm:create_header_3} through \ref{itm:demux_repeat_3} for
        \texttt{IMG\_SOLVERS\_PER\_SWITCH} times
    \item Repeat steps \ref{itm:create_header_3} through
        \ref{itm:demux_middle_repeat_3} for \texttt{PULSES} times.
\end{enumerate}

The following configurations within \textit{design/common.h} were set for the
\textit{pl\_stride} branch:

\begin{itemize}
    \item $\mathtt{PULSES} = 1$: Number of pulses to process per iteration.
    \item $\mathtt{RC\_SAMPLES} = 64$: Number of range compression samples to
        process per iteration.
    \item $\mathtt{AIE\_SWITCHES} = 1$: The number of AXI4-Stream switch
        splitter/merger pairs. The compute units that contain the AIE Switches,
        Image Reconstruction kernels, and Pixel Demux kernel are known as
        \texttt{bpCluster}'s, as shown in
        Figure~\ref{fig:aie_graph_w_stride_ctl}. NOTE: The design is currently
        only limited to an \texttt{AIE\_SWITCHES} value of 1. Future work aims
        to increase this number. See Section~\ref{subsec:pl_stride_kern} for
        more information.
    \item $\mathtt{IMG\_SOLVERS\_PER\_SWITCH} = 4$: The number of Image
        Reconstruction kernels per \texttt{bpCluster}.
\end{itemize}

\FloatBarrier


% =============================================================================
\subsubsection{Image Reconstruction Kernel}
% =============================================================================
\label{subsec:img_recon_kern}

Image Reconstruction kernels have 4 inputs and 1 output. They receive their
data from multiple sources, where two of those inputs are buffered, one of the
inputs is an AXI4-Stream packet, and one of the inputs is an RTP value from the
ARM processor. The output of the kernel is the focused image and is sent as an
AXI4-Stream packet to an AXI4-Stream merger switch implemented on the AI
Engine. See Figure~\ref{fig:aie_graph} or
Figure~\ref{fig:aie_graph_w_stride_ctl} for a visual representation of data
flow connections. The inputs and outputs for the Image Reconstruction kernel
are listed below:

\begin{itemize}
    \item \textbf{slowtime\_in (4 floats):} Buffered slowtime data containing
        the antenna X, Y, Z and range to target scene center.
    \item \textbf{rc\_in (\texttt{RC\_SAMPLES} cfloats):} Buffered range
        compressed phase data.
    \item \textbf{px\_xyz\_in (float):} AXI4-Stream packets of X, Y, and Z
        pixels of interest for the target scene. 
    \item \textbf{rtp\_dump\_img\_in (int):} RTP from ARM processor. If set to
        0, the kernel will continue cumulative sum operations for its current
        execution. If set to 1, the kernel will push the cumulative sum to the
        AXI4-Stream packet merger which then feeds into the DMA packet router
        in the PL to sort the packets into specific locations in DDR.
    \item \textbf{img\_out (cfloat):} AXI4-Stream packets of the phase
        corrected cumulative sum of a partial segment of the total target scene
        of interest.
\end{itemize}

As mentioned in Section \ref{subsec:aie_graphs}, execution of kernels begin
when they receive their specified data samples matching their allocated input
buffer size. Additionally, if an AI kernel uses synchronous RTP's for input,
then the RTP port must have data sent to it first in order to meet the
conditions for executing the kernel. For the Image Reconstruction kernel, this
means the following condition must be met for it to execute once:

\begin{itemize}
    \item \texttt{slowtime\_in} buffer contains 4 floats from the Data
        Broadcast kernel
    \item \texttt{rc\_in} buffer contains \texttt{RC\_SAMPLES} cfloats from the
        Data Broadcast kernel
    \item \texttt{rtp\_dump\_img\_in} RTP is passed in from the ARM processor
\end{itemize}

\noindent Once the Image Reconstruction kernel executes, it performs the
following steps within the \textit{main} branch:

\begin{enumerate}
    \item \label{itm:stream_read} Remove the AXI4-Stream header and fetch all
        target pixels destined for the specific Image Reconstruction kernel
        through the \texttt{px\_xyz\_in} AXI4-Stream packet port. The number of
        target pixels is based on the equation \( \frac{\texttt{PULSES} \times
        \texttt{RC\_SAMPLES}}{\texttt{IMG\_SOLVERS}}\). Each element from the
        AXI4-Stream port is of type \texttt{int} and contains the X, Y, and Z
        target pixel. A \texttt{reinterpret\_cast} is performed on the X, Y,
        and Z data to cast them into floats (since all AXI4-Stream packet data
        are int types by default) and place the X, Y, and Z floats into their
        own buffers on the heap of the running AI tile. It is important to read
        the \texttt{px\_xyz\_in} AXI4-Stream packet port as quickly as possible
        since the Pixel Demux kernel wont continue distributing target pixels
        to the other Image Reconstruction kernels in the \texttt{bpCluster}
        until it has fed all the target pixels into the Image Reconstruction
        kernel that it needs.
    \item \label{itm:buffer_load} Load 16 elements of X, Y, and Z into their
        own vector registers from the target pixel buffers created in step
        \ref{itm:stream_read}.
    \item \label{itm:diff_range} Calculate the differential range from the
        target scene center from the X, Y, and Z vector registers loaded from
        step \ref{itm:buffer_load}. Processing occurs at 16 pixels per clock
        cycle.
    \item \label{itm:diff_range_idx} Divide the differential range vector from
        step \ref{itm:diff_range} by the range resolution to get an index
        vector that can later be used for fetching values from \texttt{rc\_in}
        (used in step \ref{itm:interp}). This index vector is shifted by adding
        half of \texttt{RC\_SAMPLES}; this is done to properly align with the
        values within the \texttt{rc\_in} buffer.
    \item \label{itm:ph_corr} Calculate the phase correction vector for the
        image by multiplying the differential range vector from step
        \ref{itm:diff_range} by the phase correction coefficient. NOTE: Because
        the AI Engine API \texttt{sincos\_complex} function only works if the
        domain is between $-\pi$ and $\pi$, the phase correction vector from
        step \ref{itm:ph_corr} must be scaled up/down by $2\pi$ accordingly to
        fall within $-\pi$ and $\pi$. If the domain of \texttt{sincos\_complex}
        is not between $-\pi$ and $\pi$, the output of the function will be
        saturated to -1 or 1 depending on if the input value is below $-\pi$ or
        above $\pi$ respectively.
    \item \label{itm:interp} Because step \ref{itm:diff_range_idx} results in
        an index vector with fractional numbers, both the closest lower and
        upper whole number indices are derived. These integer indices are used
        to fetch two values from the range compressed buffer (\texttt{rc\_in})
        in order to perform linear interpolation in order to provide a more
        accurate representation of the signal at the desired fractional index.
        NOTE: Indexing into the \texttt{rc\_in} buffer is done \textbf{one at a
        time} instead of using SIMD vector operations. The algorithm breaks
        convention of using SIMD vector operations since the AI Engine API only
        support single value indexing into buffers at the time of writing; this
        results in a large performance hit to the algorithm. Performance
        improvements like this are reserved for future work. See
        Section~\ref{sec:aie_algo_optimization} for more details.
    \item \label{itm:px} Take the single interpolated range compressed value
        from step \ref{itm:interp} and multiply it by the phase corrected value
        from step \ref{itm:ph_corr} to get the phase corrected pixel in the
        image.
    \item \label{itm:img} Cumulatively add the pixel from step \ref{itm:px}
        into an image buffer stored in the heap of the AI Engine tile. This
        Image buffer persists across multiple invocations of the kernel. 
    \item \label{itm:inner_loop} Repeat steps \ref{itm:interp} through
        \ref{itm:img} for a total of 16 times to loop through all of the
        indices within the index vector from step \ref{itm:diff_range_idx}.
        NOTE: See note in step \ref{itm:interp} for why this loop is needed.
    \item \label{itm:main_loop} Repeat steps \ref{itm:buffer_load} through \ref{itm:inner_loop} for a
        total of \( \frac{\texttt{PULSES} \times
        \texttt{RC\_SAMPLES}}{\texttt{IMG\_SOLVERS}}\) times for all of the X,
        Y, and Z pixels of interest within the \texttt{px\_xy\_in} buffer.
    \item A single invocation of the Image Reconstruction kernel is completed.
        However, the \texttt{img\_out} buffer (intended for transferring the
        final focused image to the ARM processor) remains unwritten at this
        point. This is because the kernel must be invoked \texttt{PULSES} times
        to fully focus the image across all pulses. In each invocation, the
        Data Broadcast kernel provides updated slowtime data and range
        compressed samples. See \ref{subsec:data_broadcasting_kern} for more
        information on the Data Broadcast kernel. During the final pulse, the
        ARM processor signals the Image Reconstruction kernel to write the
        accumulated focused image to the \texttt{img\_out} buffer for the last
        invocation by setting the \texttt{rtp\_dump\_img\_in} RTP value to 1.
\end{enumerate}

The \textit{host\_stride} and \textit{pl\_stride} branches have similar steps
as above. The only difference is step \ref{itm:stream_read} is no longer
performed and step \ref{itm:buffer_load} reads the target pixels directly from
the AXI4-Stream \texttt{px\_xyz\_in} port in 16 pixel increments. The loop in
step \ref{itm:main_loop} encompasses the above step so that 16 pixels will be
fetched from the \texttt{px\_xyz\_in} stream every iteration. This allows the
Pixel Demuxer kernel to Round-Robin 16 pixels at a time to each Image
Reconstruction kernel, then loop back to the first Image Reconstruction kernel
to repeat the round-robin until all pixels that are fed into the Pixel Demuxer
kernel are dispersed.

% =============================================================================
\subsubsection{PL DMA Packet Router Kernel}
% =============================================================================
\label{subsec:pl_pkt_router_kern}

The DMA Packet Router PL kernel is written in HLS, which allows
the developer to write C/C++ and compile the code into
synthesizable code for the FPGA. After the Image Reconstruction
kernels have been executed \texttt{PULSES} times and they send
out their focused segments of the image through \texttt{img\_out}
AXI4-Stream packets, a PL DMA Packet Router kernel will receive
the stream from each Image Reconstruction kernel within a
\texttt{bpCluster} and stitch them together in an ordered way
before placing them in DDR. This will allow the ARM processor to
read from DDR contiguously to see the total focused image from
each Image Reconstruction kernel contribution. See
Figure~\ref{fig:aie_graph} for a graph depicting the dataflow
using the DMA Packet Router PL kernel.

\noindent The inputs and outputs for the DMA Packet Router PL kernel are listed
below:

\begin{itemize}
    \item \textbf{pl\_stream\_in:} AXI4-Stream packets of the phase corrected
        cumulative sum of a partial segment of the total target scene of
        interest.
    \item \textbf{ddr\_mem:} Pointer to the 64bit DDR memory address space.
\end{itemize}

\noindent The DMA Packet Router PL kernel performs the following steps:

\begin{enumerate}
    \item \label{itm:pl_rtr_outer_loop} Read in first 128 bits of the
        AXI4-Stream packet, which will contain the packet header metadata.
        NOTE: The first 32 bits of this packet header contains the packet id,
        packet type, AI Engine tile source row, AI Engine tile source column,
        and the odd parity of bits 0-30. See \cite{AMD_Versal_UG1079} for more
        information on this packet header. The next 32 bits is the instance ID
        of the kernel that generated the AXI4-Stream packet. Bits 64-128 are
        padding and ignored. In the future, this could be used for additional
        metadata.
    \item \label{itm:pl_rtr_ddr_offset} Calculate the DDR offset location for
        placing the AXI4-Stream packet by taking the instance ID from the
        header, and multiplying it by the number of samples (\(
        \frac{\texttt{PULSES} \times
        \texttt{RC\_SAMPLES}}{\texttt{IMG\_SOLVERS}}\)) given to each Image
        Reconstruction kernel.
    \item \label{itm:pl_rtr_inner_loop} Read another 128 bits of the
        AXI4-Stream packet and place that image data at the specified offset
        derived in step \ref{itm:pl_rtr_ddr_offset}.
    \item \label{itm:pl_rtr_repeat} Repeat step \ref{itm:pl_rtr_inner_loop} for
        \( \frac{\texttt{PULSES} \times
        \texttt{RC\_SAMPLES}}{\texttt{IMG\_SOLVERS}}\) times
    \item Repeat steps \ref{itm:pl_rtr_outer_loop} through
        \ref{itm:pl_rtr_repeat} for \texttt{IMG\_SOLVERS\_PER\_SWITCH} times
\end{enumerate}

In order for this PL kernel to be integrated into the Vivado design and a
complete XSA file be generated, the Vitis toolchain requires a system.cfg file
that specifies how the PL kernel is connected to the rest of the system. Prior
to the PL kernel being built, the Makefile will automatically update this
\textit{design/system\_cfg/system.cfg} file based on how the design is
configured within the \textit{design/common.h}.

% =============================================================================
\subsubsection{PL DMA Stride Controller Kernel}
% =============================================================================
\label{subsec:pl_stride_kern}

Similar to the DMA Packet Router PL kernel, the DMA Stride Controller PL kernel
is written in HLS. This PL kernel is only utilized within the
\textit{pl\_stride} branch and is only used for feeding the Pixel Demux kernel
with pre-sorted target pixel data from DDR. This has the following
implications:

\begin{enumerate}
    \item The Pixel Demux kernel Round-Robins 16 target pixels at a time to
        each Image Reconstruction kernel within the \texttt{bpCluster} until
        there are no more target pixels. This is as opposed to dumping the
        entire set of pixels destined for a specific Image Reconstruction
        kernel all at once.
    \item The Image Reconstruction kernels can start work as soon as receiving
        16 target pixels as opposed to waiting for all target pixels to be
        collected.
    \item More Image Reconstruction kernels can begin working in parallel
        sooner since the Image Reconstruction kernels do not need to wait for
        the Pixel Demux kernel to fully feed the prior Image Reconstruction
        kernel with all its required target pixels at one time.
\end{enumerate}

\noindent The inputs and outputs for the DMA Stride Controller PL kernel are
listed below:

\begin{itemize}
    \item \textbf{ddr\_mem:} Pointer to the 64bit DDR memory address space.
    \item \textbf{pl\_stream\_out:} AXI4-Stream of the target pixels extracted
        from DDR memory. This AXI4-Stream feeds into the Pixel Demux kernel.
\end{itemize}

\noindent The role of the DMA Stride Controller PL kernel performs the following steps:

\begin{enumerate}
    \item \label{itm:pl_stride_inner} From the target pixel array initially
        passed in via the ARM processor, read in the target pixel designated at
        the specified target pixel array DDR memory pointer (which is index 0
        of the array pointer on the first iteration) and write that to the
        \texttt{pl\_stream\_out} AXI4-Stream. NOTE: 128 bits are read since DDR
        operates on 64 bit boundaries. This means that the first 64 bits
        contain the X and Y component of the target pixel, and the second 64
        bits contains the Z and a zero pad value.
    \item Repeat step \ref{itm:pl_stride_inner} a total of 16 times to send 16
        target pixels through \texttt{pl\_stream\_out} AXI4-Stream.
    \item \label{itm:pl_stride_middle} Shift the target pixel array DDR memory
        pointer by the number of target pixels destined for each AIE kernel per
        pulse. This equates to shifting the target pixel pointer by
        \(\frac{\texttt{TOTAL\_TARGET\_PIXELS}}{\texttt{IMG\_SOLVERS}}\)
        where: \texttt{TOTAL\_TARGET\_PIXELS} = \(\texttt{AZ\_SAMPLES} \times
        \texttt{RC\_SAMPLES}\).
    \item Repeat steps \ref{itm:pl_stride_inner} through
        \ref{itm:pl_stride_middle} for total of
        \texttt{IMG\_SOLVERS\_PER\_SWITCH} times, which is the number of Image
        Reconstruction kernels that are connected to the AXI4-Stream packet
        splitter switch. This is also the number of Image Reconstruction
        kernels per \texttt{bpCluster}.
    \item \label{itm:pl_stride_outer} At this step, the target pixel array DDR
        memory pointer is at the end of the target pixel array for a single
        \texttt{bpCluster}, so the pointer rolls back back to the beginning of
        the list, but offset by 16 pixels. For example, on the first iteration
        the base index pointer was 0, on the second iteration it would be 16,
        on the third it would be 32, on the fourth it would be 48, and so on.
    \item Repeat steps \ref{itm:pl_stride_inner} through
        \ref{itm:pl_stride_outer} for a total of how many multiples of 16
        pixels it takes to get through all the target pixels destined for a
        single Image Reconstruction kernel. This equates to shifting the target
        pixel pointer by
        \(\frac{\texttt{TOTAL\_TARGET\_PIXELS}}{\texttt{IMG\_SOLVERS} \times
        16}\).
    \item \label{itm:pl_stride_sw_loop} Shift the target pixel array DDR memory
        pointer by one whole \texttt{bpCluster}'s worth of target pixels. This would
        equate to shifting the target pixel pointer by
        \(\frac{\texttt{TOTAL\_TARGET\_PIXELS}}{\texttt{AIE\_SWITCHES}}\).
    \item Repeat steps \ref{itm:pl_stride_inner} through
        \ref{itm:pl_stride_sw_loop} for a total of \texttt{AIE\_SWITCHES} (i.e.
        the number of \texttt{bpCluster} units).
    \item \label{itm:pl_stride_pulse_loop} Shift the target pixel array DDR
        memory pointer by one whole pulse worth of target pixels.
    \item Repeat steps \ref{itm:pl_stride_inner} through
        \ref{itm:pl_stride_pulse_loop} for the total number of pulses
\end{enumerate}

It is worth noting that the current design is limited to a
\texttt{AIE\_SWITCHES} value of 1. Future work aims to increase this number
(see Section~\ref{sec:dma_strd_ctlr_pl_kern_improve} for more information). In
order for this PL kernel to be integrated into the Vivado design and a complete
XSA file be generated, the Vitis toolchain requires a system.cfg file that
specifies how the PL kernel is connected to the rest of the system. Prior to
the PL kernel being built, the Makefile will automatically update this
\textit{design/system\_cfg/system.cfg} file based on how the design is
configured within the \textit{design/common.h}.

% =============================================================================
\subsection{AI Engine Graph Implementation}
% =============================================================================
\label{subsec:graph_impl}

An AI Engine design must include a dataflow graph that guides the AI Engine
compiler in interconnecting kernels internally and linking them to external
domains within the Versal chip. For more information on AI Engine graphs, see
Section \ref{subsec:aie_graphs}.

The general flow of data originates from the ARM processor. The ARM processor
provides the range compressed samples and the slow time data, which consists of
the antenna X, Y, and Z coordinates, as well as the range to target scene from
the antenna. In order to reduce the number of GMIO inputs being used (max of
32), all slow time data is sent through the same GMIO input port. The graph is
configured to forward this data to the Data Broadcast kernel and that graph
code is written to broadcast the Data Broadcast kernel output to all Image
Reconstruction kernels. \textit{Vitis Analyzer} was used to generate a block
diagram of the designed AI Engine graph, which can be seen in
Figure~\ref{fig:aie_graph} and \ref{fig:aie_graph_w_stride_ctl}.




